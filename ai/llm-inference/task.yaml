name: llm-inference
display_name: "LLM Text Generation"
category: ai
tags: [llm, text-generation, chat, completion, transformers]
hardware: [gpu]
gpu_required: true
mode: long_running
avg_latency: "20-50 tokens/s depending on model & hardware"
memory_mb: 8000
description: "Run large language models for text generation, chat, and completion on your own GPUs."
long_description: |
  Run large language models locally for text generation, chat, completion, and summarization.
  Workers keep the model loaded in GPU memory for fast inference with no cold starts.

  ## Supported Models
  - Llama 3, Mistral, Phi-3, Dolphin, Qwen (via llama.cpp GGUF)
  - Any HuggingFace model via vLLM or Transformers

  ## Features
  - Streaming token output via callbacks
  - Configurable generation parameters (temperature, top_p, max_tokens)
  - Multiple model formats (GGUF, safetensors, GPTQ, AWQ)
  - Concurrent request handling with batched inference

  ## Performance
  - 20-50 tokens/sec depending on model size and GPU
  - RTX 3060+ recommended, RTX 4090 ideal for 13B+ models
  - 7B models: ~40 tok/s on RTX 4090
  - 70B models: require 2x 24GB GPUs or quantized variants
models: ["Llama 3", "Mistral", "Phi-3", "Dolphin", "Qwen"]
tools: ["vLLM", "llama.cpp", "Transformers"]
use_cases:
  - Chatbots and conversational AI
  - Content generation
  - Code completion
  - Text summarization
input_example: |
  {"prompt": "Explain quantum computing in simple terms", "max_tokens": 256, "temperature": 0.7}
output_example: |
  {"text": "Quantum computing uses quantum bits...", "tokens_generated": 142, "tokens_per_sec": 38.5}
